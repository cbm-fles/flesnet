#!/usr/bin/env python3

# Control configuration and data taking on mFLES
#
# 2018-11-06 Jan de Cuveland <cuveland@compeng.uni-frankfurt.de>
# 2018-11-06 Dirk Hutter <hutter@compeng.uni-frankfurt.de>

"""
flesctl
Usage:
  flesctl list
  flesctl add <config_file> <tag>
  flesctl show <tag>
  flesctl start <tag>
  flesctl stop
  flesctl monitor | mon
  flesctl status
  flesctl logbook
  flesctl info <run>
  flesctl -h | --help
  flesctl --version
"""

import os
import pwd
import sys
import docopt
import glob
import subprocess
import shutil
import configparser
import time
import datetime
import elog
import signal
import requests

import inspect
import pprint

scriptdir = os.path.dirname(os.path.realpath(__file__))
confdir = os.path.normpath("/home/flesctl/config")
rundir_base = "/home/flesctl/run"
flesctl_conf = "/home/flesctl/private/flesctl.conf"
log_template = "/home/flesctl/private/logbook.template"
flesctl_syslog = "/var/log/remote_serv/flesctl.log"

# check if run as correct user
run_user = "flesctl"
username = pwd.getpwuid(os.getuid()).pw_name
sudo_user = os.environ.get("SUDO_USER")
if sudo_user == None or username != run_user:
  print("start using sudo as user", run_user)
  sys.exit()

# read global configuration
config = configparser.ConfigParser()
config.read(flesctl_conf)
run_id = config["DEFAULT"].getint("NextRunID")
if not run_id:
  print("error: no configuration at", flesctl_conf)
  sys.exit(1)
reservation = config["DEFAULT"].get("Reservation", None)

# elog configuration
log_host = 'http://mcbmgw01:8080/mCBM/'
log_attr_static = {'author': 'flesctl', 'type': 'Routine', 'category': 'mFLES'}

# mattermost configuration
mattermost_host = "https://mattermost.web.cern.ch/hooks/8i895g6rmjbqdjop3m6ee3t4qo"
mattermost_static = {"channel": "mfles-status", "username": "flesctl"}

def tags():
  for filename in glob.iglob(confdir + '/**/*.conf', recursive=True):
    if not filename.startswith(confdir + '/'):
      throw
    tag, _ = os.path.splitext(filename[len(confdir + '/'):])
    yield tag


def list_tags():
  for tag in tags():
    print(tag)


def add(tag, filename):
  # check if tag is new
  if tag in tags():
    print("error: tag already exists")
    sys.exit(1)

  # TODO: check if config is duplicate, warn

  # check syntax of new config file by running bash on it
  if subprocess.call(["/bin/bash", filename]) != 0:
    print("error: cannot parse", filename)
    sys.exit(1)
  print("adding new tag", tag)

  # cp filename /opt/flesctl/config/tag
  destpath = os.path.join(confdir, tag + ".conf")
  os.makedirs(os.path.dirname(destpath), exist_ok=True)
  shutil.copy(filename, destpath)

def print_config(tag):
  # check if tag exists
  if tag not in tags():
    print("error: tag unknown")
    sys.exit(1)
  print("# Tag:", tag)
  with open(os.path.join(confdir, tag + ".conf"), 'r') as cfile:
    print(cfile.read())


def start(tag):
  # check if readout is active
  output = subprocess.check_output(["/usr/bin/squeue", "-h", "-u", run_user],
                                   universal_newlines=True)
  if len(output) > 0:
    print("error: job is already active")
    print(output, end='')
    sys.exit(1)

  # check if tag exists
  if tag not in tags():
    print("error: tag unknown")
    sys.exit(1)

  # create run directory
  rundir = os.path.join(rundir_base, str(run_id))
  try:
    os.mkdir(rundir)
  except FileExistsError:
    print("error: run directory", rundir, "exists")
    sys.exit(1)
  os.chdir(rundir)
  os.mkdir("log")

  # increment next run id
  config["DEFAULT"]["NextRunID"] = str(run_id + 1)
  with open(flesctl_conf, 'w') as configfile:
    config.write(configfile)
  print("starting run with id", run_id)

  # TODO: check prerequisites, e.g. leftovers from previous runs

  # create run-local copy of tag config
  shutil.copy(os.path.join(confdir, tag + ".conf"), "readout.conf")

  # create run configuration file
  start_time = time.time()
  runconf = configparser.ConfigParser()
  runconf['DEFAULT'] = {'Tag': tag, 'RunId': str(run_id),
                        'StartTime': str(int(start_time))}
  with open("run.conf", "w") as runconffile:
    runconf.write(runconffile)

  # create spm and flesnet configuration from tag
  subprocess.call([os.path.join(scriptdir, "init_run"), "readout.conf", str(run_id)])

  # initialize logbook
  shutil.copy(log_template, "logbook.txt")

  # start run using spm
  cmd = ["/opt/spm/spm-run", "--batch", "--logfile", "slurm.out",
         "--jobname", "run_{}".format(run_id)]
  if reservation:
    cmd += ["--reservation", reservation]
  subprocess.call(cmd + ["readout.spm"]);

  # create elog entry
  log_msg = "Run started at {}\n".format(
    time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)))
  log_msg += " Tag: {}".format(tag)
  try:
    logbook = elog.open(log_host)
    log_id_start = logbook.post(log_msg, attributes=log_attr_static,
                                RunNumber=run_id, subject='Run start')
  except:
    print("Error: electronic logbook not reachable")
    log_id_start = -1

  runconf['DEFAULT']['elog_id_start'] = str(log_id_start)
  with open("run.conf", "w") as runconffile:
    runconf.write(runconffile)

  # create mattermost message
  mattermost_msg = ":white_check_mark:  Run {} started with tag '{}'".format(run_id, tag)
  mattermost_data = mattermost_static.copy()
  mattermost_data["text"] = mattermost_msg
  try:
    requests.post(url = mattermost_host, json = mattermost_data)
  except requests.exceptions.RequestException as e:
    print("Error: Mattermost exception: {}".format(e))


def current_run_id():
  output = subprocess.check_output(["/usr/bin/squeue", "-h", "-o", "%j", "-u", run_user],
                                   universal_newlines=True)
  if output.startswith("run_"):
    return int(output[len("run_"):])
  else:
    return None


def start_virgo_arch():
  nodes_nb =""
#  nodes_nb ="2cn"
#  nodes_nb ="7cn"

  run_id = current_run_id()
  if run_id is None:
    print("error: no run job active")
    sys.exit(1)

  # Trigger archivers clients on Virgo from node[8-9] or node[3-9] to lustre
  # This uses an ssh key with restricted command association:
  # 1) the key on mFLES allows flesctl to trigger slurm jobs on Virgo under the ploizeau account on
  #    lxbk0908 in the cbm_online partition.
  # 2) Each Job connects a tsclient archiver to the corresponding mFLES node
  try:
    output = subprocess.check_output(["/usr/bin/ssh", "-o", "GSSAPIAuthentication=no",
                                      "-o", "PasswordAuthentication=no",
                                      "-i", "~/.ssh/id_rsa_flesctl_virgo", "ploizeau@virgo.hpc",
                                      "start", "{}".format(run_id), "{}".format(nodes_nb)],
                                     universal_newlines=True)
  except subprocess.CalledProcessError as err:
    if 1 == err.returncode:
      print("Wrong parameters to the virgo archiver command, should never happen!")
      sys.exit(1)
    elif 2 == err.returncode:
      print("Slurm Job starting for the archivers on Virgo failed!")
      print(err.output)
      sys.exit(1)

  print(output)

  # Managed to start the slurm jobs on Virgo
  test_file_name = os.path.join(rundir_base, str(run_id), "virgo_arch.jobid")
  test_file = open(test_file_name, "w")
  test_file.write(output)
  test_file.close()


def stop_virgo_arch():
  # Check if archiving was started on Virgo+Lustre => Maybe not needed?
  run_id = current_run_id()
  if run_id is None:
    print("error: no run job active")
    sys.exit(1)

  test_file_virgo = os.path.join(rundir_base, str(run_id), "virgo_arch.jobid")
  if not os.path.isfile(test_file_virgo):
    print("error: archivers not running on Virgo, nothing to do")
    sys.exit(1)

  # Stops archivers clients on Virgo from node[8-9] or node[3-9] to lustre
  # This uses an ssh key with restricted command association:
  #    the key on mFLES allows flesctl to trigger the cancellation of slurm jobs on Virgo
  #    under the ploizeau account based on some Job ID fiel created when they were started.
  subprocess.call(["/usr/bin/ssh", "-o", "GSSAPIAuthentication=no",
                   "-o", "PasswordAuthentication=no",
                   "-i", "~/.ssh/id_rsa_flesctl_virgo", "ploizeau@virgo.hpc",
                   "stop"])


def stop():
  run_id = current_run_id()
  if run_id is None:
    print("error: no run job active")
    sys.exit(1)

  # change to run directory
  rundir = os.path.join(rundir_base, str(run_id))
  os.chdir(rundir)

  # Check if archiving was started on Virgo+Lustre
  test_file_virgo = os.path.join(rundir_base, str(run_id), "virgo_arch.jobid")
  if os.path.isfile(test_file_virgo):
    print("Archivers running on Virgo: stopping them")
    stopvirgoarch()

  print("stopping run with id", run_id)
  subprocess.call(["/usr/bin/scancel", "--jobname", "run_{}".format(run_id)])
  stop_time = time.time()

  # read run configuration file
  runconf = configparser.ConfigParser()
  runconf.read("run.conf")
  start_time = int(runconf['DEFAULT']['StartTime'])
  log_id_start = int(runconf['DEFAULT']['elog_id_start'])

  # create elog entry
  log_msg = "Run stopped at {}\n".format(
    time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stop_time)))
  log_msg += " Duration: {}".format(str(datetime.timedelta(seconds=(int(stop_time) - start_time))))
  try:
    logbook = elog.open(log_host)
    # do not reply if elog failed during start
    if log_id_start == -1:
      log_id_stop = logbook.post(log_msg,
                                 attributes=log_attr_static, RunNumber=run_id, subject='Run stop')
    else:
      log_id_stop = logbook.post(log_msg, msg_id=log_id_start, reply=True,
                                 attributes=log_attr_static, RunNumber=run_id, subject='Run stop')
  except:
    print("Error: electronic logbook not reachable")
    log_id_stop = -1

  # update run configuration file
  runconf['DEFAULT']['StopTime'] = str(int(stop_time))
  runconf['DEFAULT']['elog_id_stop'] = str(log_id_stop)
  with open("run.conf", "w") as runconffile:
    runconf.write(runconffile)

  # create mattermost message
  mattermost_msg = ":stop_sign:  Run {} stopped after {}".format(
    run_id, str(datetime.timedelta(seconds=(int(stop_time) - start_time))))
  mattermost_data = mattermost_static.copy()
  mattermost_data["text"] = mattermost_msg
  try:
    requests.post(url = mattermost_host, json = mattermost_data)
  except requests.exceptions.RequestException as e:
    print("Error: Mattermost exception: {}".format(e))

  # TODO: cleanup, remove leftovers


def status():
  output = subprocess.check_output(["/usr/bin/squeue", "-h", "--user",
                                    run_user], universal_newlines=True)
  if len(output) == 0:
    print("info: no active run found")
  else:
    print(output, end='')


def monitor():
  syslog_mon = True
  if syslog_mon :
    # syslog based monitoring
    signal.signal(signal.SIGINT, signal.SIG_IGN)
    subprocess.call(["/usr/bin/less", "-n",  "+G", "+F", flesctl_syslog])
  else:
    # slurm log based monitoring
    run_id = current_run_id()
    if run_id is None:
      print("error: no active run found")
      sys.exit(1)
    subprocess.call(["/usr/bin/tail", "-f", "/home/flesctl/run/{}/slurm.out".format(run_id)])


def edit_logbook():
  # TODO: add line with modifiy info like: new entry on "date"
  rundir = os.path.join(rundir_base, str(run_id-1))
  filename = os.path.join(rundir, "logbook.txt")
  # open file and jump to last line
  subprocess.call(["nano +$(wc -l \"{}\")".format(filename)], shell=True)


def run_info(par_run_id):
  config_file = os.path.join(rundir_base, par_run_id, "run.conf")
  if not os.path.isfile(config_file):
    print("error: no such run found")
    sys.exit(1)

  runconf = configparser.ConfigParser()
  runconf.read(config_file)
  tag = runconf["DEFAULT"].get("tag", None)
  starttime = runconf["DEFAULT"].getint("starttime", None)
  stoptime = runconf["DEFAULT"].getint("stoptime", None)
  start_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(starttime))
  stop_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(stoptime))
  duration = str(datetime.timedelta(seconds=(stoptime - starttime)))
  print("Run number:       ", par_run_id)
  print("Configuration tag:", tag)
  print("Started at:       ", start_str)
  print("Stopped at:       ", stop_str)
  print("Duration:         ", duration)


def copy_lustre(par_run_id):
  # check if readout is active
  output = subprocess.check_output(["/usr/bin/squeue", "-h", "--user",
                                    run_user], universal_newlines=True)
  if len(output) > 0:
    print("error: active run found: ")
    print(output, end='')
    sys.exit(1)

  # Check if run is existing
  config_file = os.path.join(rundir_base, par_run_id, "run.conf")
  if not os.path.isfile(config_file):
    print("error: no such run found")
    sys.exit(1)
  run_id = int(par_run_id)

  # change to run directory
  rundir = os.path.join(rundir_base, par_run_id)
  os.chdir(rundir)

  # Block flesctl runs during the copy process by starting empty jobs with spm-run
  # This is needed because the copy command is not running through the mFLES Slurm!
  subprocess.call([os.path.join(scriptdir, "init_copyblock"), "node[8-9]", str(run_id)])
  cmd = ["/opt/spm/spm-run", "--batch", "--logfile", "slurm_copyblock.out",
         "--jobname", "copy_{}".format(run_id)]
  if reservation:
    cmd += ["--reservation", reservation]
  subprocess.call(cmd + ["copyblock.spm"]);

  # Trigger copy on Virgo from node[8-9] or node[3-9] to lustre
  # This uses two ssh keys with restricted command association:
  # 1) the key on mFLES allows flesctl to trigger slurm jobs on Virgo under the ploizeau account on
  #    lxbk0908 in the cbm_online partition. Each Job rsyncs through infiniband from an mFLES node
  # 2) the key on Virgo allows the ploizeau account to passwordless rsync from /local/data through
  #    Infiniband from lxbk0908 and under the mFLES user loizeau (could be changed to flesctl)
  subprocess.call(["/usr/bin/ssh", "-o", "GSSAPIAuthentication=no",
                   "-o", "PasswordAuthentication=no", "-i", "~/.ssh/id_rsa_mfles_copy",
                   "ploizeau@virgo.hpc", "start", "{}".format(run_id)])

  # Remove the empty processes once the copy is done, allows checksum to run
  subprocess.call(["/usr/bin/scancel", "--jobname", "copy_{}".format(run_id)])

  # start md5 checksum and store output in the run folder using spm
  # useful for later cross-checks

  # create spm
  # TODO: list of node should be configurable if we get more nodes with archiving capability
  subprocess.call([os.path.join(scriptdir, "init_checksum"), "node[8-9]", str(run_id)])

  cmd = ["/opt/spm/spm-run", "--logfile", "slurm_checksum.out",
         "--jobname", "md5_{}".format(run_id)]
  if reservation:
    cmd += ["--reservation", reservation]
  subprocess.call(cmd + ["checksum.spm"]);

  '''
  # create elog entry
  start_time = time.time()
  log_msg = "Run copied to lustre at {}\n".format(
    time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)))
  try:
    logbook = elog.open(log_host)
    log_id_copy = logbook.post(log_msg, attributes=log_attr_static,
                                RunNumber=run_id, subject='Run copy')
  except:
    print("Error: electronic logbook not reachable")
    log_id_copy = -1

  # create mattermost message
  mattermost_msg = ":arrow_right:  Run {} copied to lustre".format(run_id)
  mattermost_data = mattermost_static.copy()
  mattermost_data["text"] = mattermost_msg
  try:
    requests.post(url = mattermost_host, json = mattermost_data)
  except requests.exceptions.RequestException as e:
    print("Error: Mattermost exception: {}".format(e))
  '''

def disk_space(par_nodes):
  # start script formatting df output on each requested node using spm
  subprocess.call([os.path.join(scriptdir, "init_diskspace"), par_nodes])
  cmd = ["/opt/spm/spm-run", "--logfile", "/scratch/diskspace_slurm.out",
         "--jobname", "disk_space"]
  if reservation:
    cmd += ["--reservation", reservation]
  subprocess.call(cmd + ["/scratch/diskspace.spm"]);
  #subprocess.call(["/bin/grep", "Left", "/scratch/diskspace.out"])


def delete_tsa(par_run_id):
  # check if readout is active
  output = subprocess.check_output(["/usr/bin/squeue", "-h", "--user",
                                    run_user], universal_newlines=True)
  if len(output) > 0:
    print("error: active run found: ")
    print(output, end='')
    sys.exit(1)

  # Check if run is existing
  config_file = os.path.join(rundir_base, par_run_id, "run.conf")
  if not os.path.isfile(config_file):
    print("error: no such run found")
    sys.exit(1)
  run_id = int(par_run_id)

  # change to run directory
  rundir = os.path.join(rundir_base, par_run_id)
  os.chdir(rundir)

  # Checksum check against lustre copy
  # Trigger dry-run copy with checksum on Virgo from node[8-9] or node[3-9] to lustre
  # This uses two ssh keys with restricted command association:
  # 1) the key on mFLES allows flesctl to trigger slurm jobs on Virgo under the ploizeau account on
  #    lxbk0908 in the cbm_online partition. Each Job rsyncs through infiniband from an mFLES node
  # 2) the key on Virgo allows the ploizeau account to passwordless rsync from /local/data through
  #    Infiniband from lxbk0908 and under the mFLES user loizeau (could be changed to flesctl)
  try:
  checkout = subprocess.check_output(["/usr/bin/ssh", "-o", "GSSAPIAuthentication=no",
                                      "-o", "PasswordAuthentication=no",
                                      "-i", "~/.ssh/id_rsa_mfles_copy",
                                        "ploizeau@virgo.hpc", "start", "{}".format(run_id)],
                                       universal_newlines=True)
  except subprocess.CalledProcessError as err:
    if 1 == err.returncode:
      print("Wrong parameters to the checksum command, should never happen!")
    sys.exit(1)
    elif 2 == err.returncode:
    print("Files not found on mFLES, the run was probably already partially or fully removed!")
      print( err.output )
    sys.exit(1)
    elif 3 == err.returncode:
    print("Some of the files failed the cheksum, new copy is needed!")
      print( err.output )
    sys.exit(1)

  # Checksum ok, delete corresponding TSA files on all nodes using spm
  # TODO: list of node should be configurable if we get more nodes with archiving capability
  subprocess.call([os.path.join(scriptdir, "init_delete_tsa"), "node[8-9]", str(run_id)])
  cmd = ["/opt/spm/spm-run", "--logfile", "tsa_delete.out",
         "--jobname", "tsadel_{}".format(run_id)]
  if reservation:
    cmd += ["--reservation", reservation]
  subprocess.call(cmd + ["delete.spm"]);

  '''
  # create elog entry
  log_msg = "Run tsa removed from local storage at {}\n".format(
    time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(start_time)))
  try:
    logbook = elog.open(log_host)
    log_id_copy = logbook.post(log_msg, attributes=log_attr_static,
                                RunNumber=run_id, subject='Run local tsa removed')
  except:
    print("Error: electronic logbook not reachable")
    log_id_copy = -1

  # create mattermost message
  mattermost_msg = ":warning:  Run {} tsa removed from local storage".format(run_id)
  mattermost_data = mattermost_static.copy()
  mattermost_data["text"] = mattermost_msg
  try:
    requests.post(url = mattermost_host, json = mattermost_data)
  except requests.exceptions.RequestException as e:
    print("Error: Mattermost exception: {}".format(e))
  '''

if __name__ == "__main__":
  arg = docopt.docopt(__doc__, version='0.2')

  if arg['list']:
    list_tags()

  if arg['add']:
    add(arg['<tag>'], arg['<config_file>'])

  if arg['show']:
    print_config(arg['<tag>'])

  if arg['start']:
    start(arg['<tag>'])

  if arg['startvirgoarch']:
    start_virgo_arch()

  if arg['stopvirgoarch']:
    stop_virgo_arch()

  if arg['stop']:
    stop()

  if arg['status']:
    status()

  if arg['monitor'] or arg['mon']:
    monitor()

  if arg['logbook']:
    edit_logbook()

  if arg['info']:
    run_info(arg['<run>'])

  if arg['copy']:
    copy_lustre(arg['<run>'])

  if arg['disk']:
    disk_space(arg['<node(s)>'])

  if arg['freetsa']:
    delete_tsa(arg['<run>'])
